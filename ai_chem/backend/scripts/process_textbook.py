"""
æ•™æPDFè§£æè„šæœ¬
ä»PDFæ•™æä¸­æå–æ–‡æœ¬å†…å®¹ï¼ŒæŒ‰ç« èŠ‚åˆ†å—ï¼Œä¿å­˜ä¸ºç»“æ„åŒ–JSON
"""
import json
import re
import sys
import os
from pathlib import Path
from typing import List, Dict, Any, Optional

# è®¾ç½®æ§åˆ¶å°ç¼–ç ä¸º UTF-8ï¼ˆWindows å…¼å®¹ï¼‰
if sys.platform == "win32":
    try:
        import codecs
        sys.stdout = codecs.getwriter("utf-8")(sys.stdout.buffer, "strict")
        sys.stderr = codecs.getwriter("utf-8")(sys.stderr.buffer, "strict")
    except:
        os.system("chcp 65001 > nul 2>&1")


def safe_import_unstructured():
    """å®‰å…¨å¯¼å…¥ PDF è§£æåº“ï¼ŒæŒ‰ç¨³å®šæ€§æ’åº"""
    # ä¼˜å…ˆä½¿ç”¨ pdfplumber (æœ€ç¨³å®šï¼Œæ— é¢å¤–ä¾èµ–)
    try:
        import pdfplumber
        return pdfplumber, "pdfplumber"
    except ImportError:
        pass

    # å¤‡é€‰ï¼šä½¿ç”¨ PyPDF2
    try:
        import PyPDF2
        return PyPDF2, "pypdf2"
    except ImportError:
        pass

    # æœ€åå°è¯• unstructured (å¯èƒ½æœ‰ nltk ç­‰ä¾èµ–é—®é¢˜)
    try:
        from unstructured.partition.pdf import partition_pdf
        return partition_pdf, "unstructured"
    except ImportError:
        pass

    return None, None


# å¯¼å…¥PDFè§£æåº“
partition_pdf, LIB_TYPE = safe_import_unstructured()


def get_element_text(element) -> str:
    """ç»Ÿä¸€è·å–å…ƒç´ æ–‡æœ¬çš„æ¥å£"""
    if LIB_TYPE == "unstructured":
        return str(element) if element else ""
    elif LIB_TYPE == "pdfplumber":
        return element.get("text", "") if isinstance(element, dict) else str(element)
    else:
        return str(element) if element else ""


def get_element_category(element) -> str:
    """è·å–å…ƒç´ ç±»åˆ«"""
    if LIB_TYPE == "unstructured":
        return getattr(element, "category", "")
    else:
        return ""


def clean_text(text: str) -> str:
    """æ¸…ç†æ–‡æœ¬ï¼šå»é™¤å¤šä½™ç©ºç™½ã€æ ‡å‡†åŒ–åŒ–å­¦å¼"""
    if not text:
        return ""

    # å»é™¤å¤šä½™ç©ºç™½
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()

    # æ ‡å‡†åŒ–åŒ–å­¦å¼ä¸‹æ ‡ (å¦‚ Hâ‚‚O â†’ H2O)
    # æ³¨æ„ï¼šè¿™ä¸ªç®€å•æ›¿æ¢å¯èƒ½ä¸å¤Ÿå®Œå–„ï¼Œå®é™…å¯èƒ½éœ€è¦æ›´å¤æ‚çš„å¤„ç†
    text = re.sub(r'â‚€', '0', text)
    text = re.sub(r'â‚', '1', text)
    text = re.sub(r'â‚‚', '2', text)
    text = re.sub(r'â‚ƒ', '3', text)
    text = re.sub(r'â‚„', '4', text)
    text = re.sub(r'â‚…', '5', text)
    text = re.sub(r'â‚†', '6', text)
    text = re.sub(r'â‚‡', '7', text)
    text = re.sub(r'â‚ˆ', '8', text)
    text = re.sub(r'â‚‰', '9', text)

    # æ ‡å‡†åŒ–åŒ–å­¦å¼ä¸Šæ ‡
    text = re.sub(r'â°', '0', text)
    text = re.sub(r'Â¹', '1', text)
    text = re.sub(r'Â²', '2', text)
    text = re.sub(r'Â³', '3', text)
    text = re.sub(r'â´', '4', text)
    text = re.sub(r'âµ', '5', text)
    text = re.sub(r'â¶', '6', text)
    text = re.sub(r'â·', '7', text)
    text = re.sub(r'â¸', '8', text)
    text = re.sub(r'â¹', '9', text)

    return text


def extract_chapter_number(title: str) -> str:
    """ä»æ ‡é¢˜ä¸­æå–ç« èŠ‚å·"""
    # åŒ¹é… "ç¬¬xç« " æˆ– "ç¬¬ä¸€ç« " ç­‰æ¨¡å¼
    patterns = [
        r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+)ç« ',
        r'([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d+])ã€',
        r'Chapter\s*(\d+)',
        r'(\d+)\s*[\.ã€]',
    ]

    for pattern in patterns:
        match = re.search(pattern, title)
        if match:
            num = match.group(1)
            # è½¬æ¢ä¸­æ–‡æ•°å­—
            chinese_nums = {'ä¸€': '1', 'äºŒ': '2', 'ä¸‰': '3', 'å››': '4',
                          'äº”': '5', 'å…­': '6', 'ä¸ƒ': '7', 'å…«': '8',
                          'ä¹': '9', 'å': '10'}
            return chinese_nums.get(num, num)

    return ""


def split_into_chunks(text: str, chunk_size: int = 400, overlap_sentences: int = 2) -> List[str]:
    """å°†æ–‡æœ¬åˆ†æˆå—ï¼ŒæŒ‰chunk_sizeåˆ†å—ï¼Œoverlap_sentencesä¸ºé‡å å¥å­æ•°"""
    if not text:
        return []

    # æŒ‰å¥å­åˆ†å‰²ï¼ˆä¿ç•™æ ‡ç‚¹ï¼‰
    sentences = re.split(r'([ã€‚ï¼ï¼Ÿï¼›\n])', text)
    sentences = [s + t for s, t in zip(sentences[::2], sentences[1::2] + [''])]
    sentences = [s.strip() for s in sentences if s.strip()]

    if not sentences:
        return [text] if text.strip() else []

    chunks = []
    i = 0  # å½“å‰å¥å­ç´¢å¼•

    while i < len(sentences):
        # å¼€å§‹ä¸€ä¸ªæ–°çš„chunk
        current_sentences = []
        current_size = 0

        # æ·»åŠ å¥å­ç›´åˆ°è¶…è¿‡chunk_size
        while i < len(sentences):
            sentence = sentences[i]
            sentence_tokens = len(sentence) / 1.5

            # å¦‚æœæ·»åŠ æ­¤å¥å­ä¼šè¶…é™ä¸”chunkä¸ä¸ºç©ºï¼Œåˆ™åœæ­¢
            if current_size + sentence_tokens > chunk_size and current_sentences:
                break

            current_sentences.append(sentence)
            current_size += sentence_tokens
            i += 1

        # ä¿å­˜å½“å‰chunk
        if current_sentences:
            chunks.append(''.join(current_sentences))

        # å›é€€overlap_sentencesä¸ªå¥å­ï¼Œä½œä¸ºä¸‹ä¸€å—çš„å¼€å¤´
        if i < len(sentences):
            i = max(0, i - overlap_sentences)

    return chunks


def extract_chemical_entities(text: str) -> Dict[str, List[str]]:
    """ç®€å•çš„åŒ–å­¦å®ä½“æå–"""
    entities = {
        "formulas": [],
        "elements": [],
        "compounds": []
    }

    # å¸¸è§å…ƒç´ ç¬¦å·
    common_elements = [
        'H', 'He', 'Li', 'Be', 'B', 'C', 'N', 'O', 'F', 'Ne',
        'Na', 'Mg', 'Al', 'Si', 'P', 'S', 'Cl', 'Ar', 'K', 'Ca',
        'Fe', 'Cu', 'Zn', 'Ag', 'Ba', 'Hg', 'Mn'
    ]

    # æå–åŒ–å­¦å¼ï¼ˆç®€å•æ¨¡å¼ï¼šå¤§å†™å­—æ¯+æ•°å­—/å°å†™å­—æ¯+æ•°å­—ï¼‰
    formula_pattern = r'\b[A-Z][a-z]?\d*\b'
    formulas = re.findall(formula_pattern, text)

    # è¿‡æ»¤å¹¶åˆ†ç±»
    for f in set(formulas):
        if f in common_elements:
            entities["elements"].append(f)
        elif len(f) > 1:  # å¯èƒ½æ˜¯åŒ–åˆç‰©
            entities["formulas"].append(f)

    return entities


def parse_textbook(pdf_path: Path, output_dir: Path) -> Optional[Dict[str, Any]]:
    """è§£æå•ä¸ªæ•™æPDF"""
    print(f"\n{'='*60}")
    print(f"ğŸ“– æ­£åœ¨å¤„ç†: {pdf_path.name}")
    print(f"{'='*60}")

    if partition_pdf is None:
        print(f"âŒ é”™è¯¯: æœªæ‰¾åˆ°å¯ç”¨çš„PDFè§£æåº“")
        print(f"   è¯·å®‰è£…: pip install unstructured[local-inference] pdfplumber PyPDF2")
        return None

    # è§£æPDF
    print(f"â³ æ­£åœ¨è§£æPDF (ä½¿ç”¨ {LIB_TYPE})...")

    try:
        if LIB_TYPE == "unstructured":
            elements = partition_pdf(
                filename=str(pdf_path),
                strategy="fast",  # ä½¿ç”¨ fast ç­–ç•¥ï¼Œä¸éœ€è¦ poppler
                extract_images_in_pdf=False,
                extract_tables=False,
            )
            # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
            parsed_elements = [{"text": str(e), "category": getattr(e, "category", "Text")} for e in elements]

        elif LIB_TYPE == "pdfplumber":
            parsed_elements = []
            with partition_pdf.open(pdf_path) as pdf:
                for page in pdf.pages:
                    text = page.extract_text() or ""
                    if text.strip():
                        # æŒ‰è¡Œåˆ†å‰²
                        for line in text.split('\n'):
                            line = line.strip()
                            if line:
                                parsed_elements.append({"text": line, "category": "Text"})

        elif LIB_TYPE == "pypdf2":
            parsed_elements = []
            reader = partition_pdf.PdfReader(str(pdf_path))
            for page in reader.pages:
                text = page.extract_text() or ""
                # æŒ‰è¡Œåˆ†å‰²
                for line in text.split('\n'):
                    line = line.strip()
                    if line:
                        parsed_elements.append({"text": line, "category": "Text"})

        print(f"âœ… è§£æå®Œæˆï¼Œå…± {len(parsed_elements)} ä¸ªå…ƒç´ ")

    except Exception as e:
        print(f"âŒ è§£æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

    # æå–æ–‡æœ¬å†…å®¹
    print("â³ æ­£åœ¨æå–æ–‡æœ¬å†…å®¹...")
    full_text = ""
    current_section = None
    sections = []

    for element in parsed_elements:
        text = clean_text(element.get("text", ""))
        if not text:
            continue

        # æ£€æµ‹æ ‡é¢˜ï¼ˆç®€å•åˆ¤æ–­ï¼šçŸ­ä¸”ç‹¬ç«‹æˆè¡Œï¼‰
        category = element.get("category", "")
        if category in ["Title", "Header"]:
            if current_section:
                sections.append(current_section)
            current_section = {
                "title": text,
                "content": "",
                "chunks": []
            }
        elif current_section is not None:
            current_section["content"] += text + "\n"
        else:
            full_text += text + "\n"

    # æ·»åŠ æœ€åä¸€ä¸ªsection
    if current_section:
        sections.append(current_section)

    # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°sectionï¼Œå°†å…¨æ–‡ä½œä¸ºä¸€ä¸ªsection
    if not sections:
        sections = [{"title": "å…¨æ–‡", "content": full_text, "chunks": []}]

    print(f"âœ… æå–åˆ° {len(sections)} ä¸ªç« èŠ‚/éƒ¨åˆ†")

    # åˆ†å—å¤„ç†
    print("â³ æ­£åœ¨åˆ†å—å¤„ç†...")
    chunk_id = 0
    all_chunks = []

    for section in sections:
        chunks = split_into_chunks(section["content"])
        section["chunks"] = []

        for i, chunk_text in enumerate(chunks):
            # æå–åŒ–å­¦å®ä½“
            entities = extract_chemical_entities(chunk_text)

            chunk_data = {
                "chunk_id": f"{pdf_path.stem}_chunk_{chunk_id:04d}",
                "section_title": section["title"],
                "chunk_index": i,
                "text": chunk_text,
                "entities": entities,
                "metadata": {
                    "source": pdf_path.name,
                    "section": section["title"],
                    "chunk_size": len(chunk_text),
                }
            }

            section["chunks"].append(chunk_data)
            all_chunks.append(chunk_data)
            chunk_id += 1

    print(f"âœ… åˆ†å—å®Œæˆï¼Œå…± {len(all_chunks)} ä¸ªæ–‡æœ¬å—")

    # æ„å»ºç»“æœ
    result = {
        "source": pdf_path.name,
        "source_type": "textbook",
        "total_sections": len(sections),
        "total_chunks": len(all_chunks),
        "sections": [
            {
                "title": s["title"],
                "chunk_count": len(s["chunks"]),
                "chunks": s["chunks"]
            }
            for s in sections
        ]
    }

    # ä¿å­˜JSON
    output_file = output_dir / f"{pdf_path.stem}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"ğŸ’¾ å·²ä¿å­˜åˆ°: {output_file}")

    return result


def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥PDFè§£æåº“æ˜¯å¦å¯ç”¨
    if partition_pdf is None:
        print("="*60)
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ°å¯ç”¨çš„PDFè§£æåº“ï¼")
        print("="*60)
        print("\nè¯·å®‰è£…ä»¥ä¸‹ä»»ä¸€åº“ï¼š")
        print("  pip install pdfplumber                     # æ¨è")
        print("  pip install PyPDF2                          # å¤‡é€‰")
        print("  pip install unstructured[local-inference]  # å¤‡é€‰")
        print("="*60)
        sys.exit(1)

    print(f"âœ… ä½¿ç”¨PDFè§£æåº“: {LIB_TYPE}")

    # å®šä¹‰è·¯å¾„
    raw_dir = Path("backend/data/raw/textbooks")
    output_dir = Path("backend/data/collected/textbooks")

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir.mkdir(parents=True, exist_ok=True)

    # è·å–æ‰€æœ‰PDFæ–‡ä»¶
    pdf_files = list(raw_dir.glob("*.pdf"))

    if not pdf_files:
        print("âŒ æœªæ‰¾åˆ°PDFæ–‡ä»¶ï¼Œè¯·å°†æ•™æPDFæ”¾å…¥ backend/data/raw/textbooks/ ç›®å½•")
        return

    print(f"ğŸ“š æ‰¾åˆ° {len(pdf_files)} æœ¬æ•™æ")

    # å¤„ç†æ¯æœ¬æ•™æ
    all_results = []
    for pdf_file in pdf_files:
        result = parse_textbook(pdf_file, output_dir)
        if result:
            all_results.append(result)

    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    print(f"\n{'='*60}")
    print("ğŸ“Š å¤„ç†æ±‡æ€»")
    print(f"{'='*60}")

    for result in all_results:
        print(f"\nğŸ“– {result['source']}")
        print(f"   ç« èŠ‚: {result['total_sections']}")
        print(f"   æ–‡æœ¬å—: {result['total_chunks']}")

    print(f"\n{'='*60}")
    print(f"âœ… å…¨éƒ¨å®Œæˆï¼å…±å¤„ç† {len(all_results)} æœ¬æ•™æ")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir.absolute()}")
    print(f"{'='*60}")


if __name__ == "__main__":
    main()
